{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a2336e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1695f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\jakar\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_loaded = BertForSequenceClassification.from_pretrained(\"/Users/jakar/Downloads/model/mm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/Users/jakar/Downloads/model/mm\")\n",
    "sent = \"i is a student\"\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "input_id = torch.LongTensor(input_id)\n",
    "attention_mask = torch.LongTensor(attention_mask)\n",
    "with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model_loaded(input_id, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26592a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[0]\n",
    "index = logits.argmax()\n",
    "torch_label = index.detach().cpu().numpy()\n",
    "print(torch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ddf5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68d8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"/Users/jakar/Downloads/model/bert.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c6bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(input_id),\n",
    "                        ort_session.get_inputs()[1].name: to_numpy(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8e16fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "ort_outs = ort_session.run([\"output\"], ort_inputs)\n",
    "torch_onnx_output = torch.tensor(ort_outs[0], dtype=torch.float32)\n",
    "onnx_logits = F.softmax(torch_onnx_output, dim=1)\n",
    "logits_label = torch.argmax(onnx_logits, dim=1)\n",
    "label = logits_label.detach().cpu().numpy()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3087a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
