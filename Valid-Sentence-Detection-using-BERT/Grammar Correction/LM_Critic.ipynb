{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LM Critic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ftF-kNfBCzQ",
        "outputId": "321db075-f261-478f-dbd8-03a22d6f6b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LM-Critic'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 53 (delta 3), reused 50 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/michiyasunaga/LM-Critic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFWzE4ubBjY4",
        "outputId": "081241d7-14b3-49fd-aa0e-b992a6fad5a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 30.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/LM-Critic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGOjpS2aBN2S",
        "outputId": "f396e9a0-c712-46e6-b165-25371beff1cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LM-Critic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69V76HWOB4Cc",
        "outputId": "790c8906-a5fd-484b-f49b-6469643c32a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python3 critic/critic.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MHeK7XeBSJ9",
        "outputId": "e3ed1f02-0d5d-4315-e496-9c557559a482"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded gpt2\n",
            "Enter a sentence: I like apple.\n",
            "Bad! Your sentence log(p) = -22.334\n",
            "Neighbor sentence with highest log(p): I like apples. (= -19.480)\n",
            "Enter a sentence: I like apples\n",
            "Good! Your sentence log(p) = -17.912\n",
            "Enter a sentence: ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/LM-Critic/gec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvBHFMYtDBUk",
        "outputId": "c1de9442-1910-445c-f8e3-15df4f426793"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LM-Critic/gec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download_data.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKKz3ilODake",
        "outputId": "c9bd920c-69f3-4708-dde5-cf5f0c29b571"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download_data.sh: line 1: conda: command not found\n",
            "--2022-08-10 09:21:20--  https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
            "Resolving www.comp.nus.edu.sg (www.comp.nus.edu.sg)... 45.60.35.225\n",
            "Connecting to www.comp.nus.edu.sg (www.comp.nus.edu.sg)|45.60.35.225|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 643482 (628K) [application/x-gzip]\n",
            "Saving to: ‘conll14st-test-data.tar.gz’\n",
            "\n",
            "\r          conll14st   0%[                    ]       0  --.-KB/s               \rconll14st-test-data 100%[===================>] 628.40K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-08-10 09:21:20 (19.4 MB/s) - ‘conll14st-test-data.tar.gz’ saved [643482/643482]\n",
            "\n",
            "python3: can't open file 'scripts/get_orig_from_m2.py': [Errno 2] No such file or directory\n",
            "--2022-08-10 09:21:20--  https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz\n",
            "Resolving www.cl.cam.ac.uk (www.cl.cam.ac.uk)... 128.232.0.20, 2a05:b400:110::80:14\n",
            "Connecting to www.cl.cam.ac.uk (www.cl.cam.ac.uk)|128.232.0.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6120469 (5.8M) [application/x-gzip]\n",
            "Saving to: ‘wi+locness_v2.1.bea19.tar.gz’\n",
            "\n",
            "wi+locness_v2.1.bea 100%[===================>]   5.84M  3.54MB/s    in 1.6s    \n",
            "\n",
            "2022-08-10 09:21:23 (3.54 MB/s) - ‘wi+locness_v2.1.bea19.tar.gz’ saved [6120469/6120469]\n",
            "\n",
            "python3: can't open file 'scripts/get_orig_from_m2.py': [Errno 2] No such file or directory\n",
            "Cloning into 'GMEG'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Total 100 (delta 0), reused 0 (delta 0), pack-reused 100\u001b[K\n",
            "Receiving objects: 100% (100/100), 2.08 MiB | 1.47 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "download_data.sh: line 26: errant_parallel: command not found\n",
            "download_data.sh: line 31: errant_parallel: command not found\n",
            "Cloning into 'm2scorer'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "--2022-08-10 09:21:26--  https://nlp.stanford.edu/projects/myasu/LM-Critic/data.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2397451963 (2.2G) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.23G  17.0MB/s    in 2m 25s  \n",
            "\n",
            "2022-08-10 09:23:52 (15.8 MB/s) - ‘data.zip’ saved [2397451963/2397451963]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/round0__synthetic/\n",
            "  inflating: data/round0__synthetic/synthetic_paired_data_9M.json  \n",
            "   creating: data/round0__synthetic/model-fixer/\n",
            "  inflating: data/round0__synthetic/model-fixer/merges.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/config.json  \n",
            "  inflating: data/round0__synthetic/model-fixer/vocab.json  \n",
            "   creating: data/round0__synthetic/model-fixer/predictions/\n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.wiki.eval.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/conll14.eval.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.yahoo.outm2.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/bea19dev.out.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/bea19dev.eval.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.yahoo.eval.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.wiki.outm2.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/bea19dev.outm2.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.wiki.out.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/gmeg.yahoo.out.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/predictions/conll14.out.txt  \n",
            "  inflating: data/round0__synthetic/model-fixer/tokenizer_config.json  \n",
            "  inflating: data/round0__synthetic/model-fixer/pytorch_model.bin  \n",
            "  inflating: data/round0__synthetic/model-fixer/special_tokens_map.json  \n",
            "   creating: data/round1__BIFI/\n",
            "  inflating: data/round1__BIFI/BIFI_paired_data_9M.json  \n",
            "   creating: data/round1__BIFI/model-fixer/\n",
            "  inflating: data/round1__BIFI/model-fixer/config.json  \n",
            "  inflating: data/round1__BIFI/model-fixer/merges.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/special_tokens_map.json  \n",
            "  inflating: data/round1__BIFI/model-fixer/vocab.json  \n",
            "  inflating: data/round1__BIFI/model-fixer/tokenizer_config.json  \n",
            "   creating: data/round1__BIFI/model-fixer/predictions/\n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/bea19dev.eval.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.yahoo.out.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.wiki.outm2.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.yahoo.outm2.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/bea19dev.outm2.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.yahoo.eval.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/bea19dev.out.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/conll14.out.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.wiki.out.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/conll14.eval.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/predictions/gmeg.wiki.eval.txt  \n",
            "  inflating: data/round1__BIFI/model-fixer/pytorch_model.bin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gWL5nk_KEq1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash src/run-round0.sh"
      ],
      "metadata": {
        "id": "GMn6sEA6EG_x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash src/run-round1.sh"
      ],
      "metadata": {
        "id": "GEZnuKyRElF9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LM-Critic/gec/scripts/get_corr_from_m2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hnOoEJ0FvM0",
        "outputId": "3debbc3e-8b6e-4994-9ea7-17a12f936f07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: get_corr_from_m2.py [-h] -out OUT [-id ID] m2_file\n",
            "get_corr_from_m2.py: error: the following arguments are required: m2_file, -out\n"
          ]
        }
      ]
    }
  ]
}