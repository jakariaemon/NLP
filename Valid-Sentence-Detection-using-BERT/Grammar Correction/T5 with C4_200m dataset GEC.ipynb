{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47275c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c0be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94880dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b43ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722da915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f983a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2764931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2695c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd393db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('c4_200m_550k.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d13d7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index</td>\n",
       "      <td>The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.</td>\n",
       "      <td>When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.</td>\n",
       "      <td>One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?</td>\n",
       "      <td>Answers » Regions » Is Nagorno-Karabakh region part of Armenia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flaneuring in fun at maple creek SK!</td>\n",
       "      <td>Flaneuring Fun in Maple Creek SK!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   input  \\\n",
       "0    The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index   \n",
       "1                                              When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.   \n",
       "2  One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.   \n",
       "3                                                                Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?   \n",
       "4                                                                                                   Flaneuring in fun at maple creek SK!   \n",
       "\n",
       "                                                                                                                                   output  \n",
       "0  The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.  \n",
       "1                                                  When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.  \n",
       "2         One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.  \n",
       "3                                                                         Answers » Regions » Is Nagorno-Karabakh region part of Armenia?  \n",
       "4                                                                                                       Flaneuring Fun in Maple Creek SK!  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a423c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14401dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakar\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfad4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dce8ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((495000, 2), (55000, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea87c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7443ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486496</th>\n",
       "      <td>You are My Fantasy. And My Reality. So good. ...Sated.</td>\n",
       "      <td>You are My Fantasy. And, My Reality. So good. ...Sated.</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58208</th>\n",
       "      <td>I have always found it be good with.</td>\n",
       "      <td>I have always found it to be good.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265247</th>\n",
       "      <td>The average rock you would pick up has an SG of about 2.75 because all the earth’s bulk crust is made up of quartz, calcite &amp; feldspar.</td>\n",
       "      <td>The average rock you would pick up has an SG of about 2.75. Because most of the earth’s crust is made up of quartz, calcite &amp; feldspar.</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499297</th>\n",
       "      <td>Bronzes, Mirrors and paintings, fine some art of Sydney.</td>\n",
       "      <td>Bronzes, Mirrors and Paintings, some of the finest in Sydney.</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124684</th>\n",
       "      <td>Is this how america became under Donald Trump?</td>\n",
       "      <td>Is this what America has become under Donald Trump?</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          input  \\\n",
       "486496                                                                                   You are My Fantasy. And My Reality. So good. ...Sated.   \n",
       "58208                                                                                                      I have always found it be good with.   \n",
       "265247  The average rock you would pick up has an SG of about 2.75 because all the earth’s bulk crust is made up of quartz, calcite & feldspar.   \n",
       "499297                                                                                 Bronzes, Mirrors and paintings, fine some art of Sydney.   \n",
       "124684                                                                                           Is this how america became under Donald Trump?   \n",
       "\n",
       "                                                                                                                                         output  \\\n",
       "486496                                                                                  You are My Fantasy. And, My Reality. So good. ...Sated.   \n",
       "58208                                                                                                        I have always found it to be good.   \n",
       "265247  The average rock you would pick up has an SG of about 2.75. Because most of the earth’s crust is made up of quartz, calcite & feldspar.   \n",
       "499297                                                                            Bronzes, Mirrors and Paintings, some of the finest in Sydney.   \n",
       "124684                                                                                      Is this what America has become under Donald Trump?   \n",
       "\n",
       "        input_token_len  \n",
       "486496               18  \n",
       "58208                10  \n",
       "265247               40  \n",
       "499297               15  \n",
       "124684               12  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8ab3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    55000.000000\n",
       "mean        33.536127\n",
       "std         25.961464\n",
       "min          2.000000\n",
       "25%         17.000000\n",
       "50%         27.000000\n",
       "75%         42.000000\n",
       "max        918.000000\n",
       "Name: input_token_len, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['input_token_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb84179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdea2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'input_token_len', '__index_level_0__'],\n",
       "    num_rows: 55000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c9f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "\n",
    "        # tokenize inputs\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6a5f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 20\n",
      "attention_mask 20\n",
      "labels 24\n",
      "{'input_ids': [71, 973, 24, 14079, 24067, 38, 96, 77, 221, 3728, 121, 19, 59, 2930, 7509, 640, 569, 2287, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [71, 973, 24, 14079, 3, 89, 12578, 887, 21, 96, 77, 221, 3728, 121, 3270, 19, 59, 2930, 7509, 640, 569, 2287, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
    "print(dataset[121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9a15e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\jakar\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from rouge_score) (1.2.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from rouge_score) (1.21.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\jakar\\anaconda3\\lib\\site-packages (from click->nltk->rouge_score) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a46bc579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7edd71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68abb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "device=torch.device(\"cuda:0\")\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"/Users/jakar/Documents/model\",\n",
    "                        evaluation_strategy=\"steps\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.01,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        fp16 = True,\n",
    "                        gradient_accumulation_steps = 6,\n",
    "                        eval_steps = 500,\n",
    "                        save_steps = 500,\n",
    "                        load_best_model_at_end=True,\n",
    "                        logging_dir=\"/logs\",\n",
    "                        report_to=\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1914255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "557c8ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jakar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2525af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "315baed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakar\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 495000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 6\n",
      "  Total optimization steps = 10312\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memon1521\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\jakar\\Documents\\HDK Tasks\\Grammer Error Correction\\wandb\\run-20220811_091552-2bv8kap1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/emon1521/huggingface/runs/2bv8kap1\" target=\"_blank\">/Users/jakar/Documents/model</a></strong> to <a href=\"https://wandb.ai/emon1521/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='10312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  501/10312 06:34 < 2:09:09, 1.27 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6875' max='6875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6875/6875 45:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 55000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1817\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2038\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2036\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2038\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_seq2seq.py:79\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     75\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2755\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2757\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 2758\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2759\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2761\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   2762\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   2763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2768\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   2769\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m   2770\u001b[0m     speed_metrics(\n\u001b[0;32m   2771\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2775\u001b[0m     )\n\u001b[0;32m   2776\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:3041\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3037\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3038\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3039\u001b[0m         )\n\u001b[0;32m   3040\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3041\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3043\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Replace -100 in the labels as we can't decode them.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m---> 10\u001b[0m decoded_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Rouge expects a newline after each sentence\u001b[39;00m\n\u001b[0;32m     13\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(nltk\u001b[38;5;241m.\u001b[39msent_tokenize(pred\u001b[38;5;241m.\u001b[39mstrip())) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m decoded_preds]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3328\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3307\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3310\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3311\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3326\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   3329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3330\u001b[0m             seq,\n\u001b[0;32m   3331\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3332\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3334\u001b[0m         )\n\u001b[0;32m   3335\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3336\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[0;32m   3306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3307\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3310\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3311\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3326\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 3329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   3330\u001b[0m             seq,\n\u001b[0;32m   3331\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3332\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3334\u001b[0m         )\n\u001b[0;32m   3335\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[0;32m   3336\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3367\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   3368\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   3369\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3370\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3371\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3372\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    923\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    928\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 931\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[0;32m    936\u001b[0m     sub_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py:907\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m    906\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index)\n\u001b[1;32m--> 907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_ids\u001b[49m:\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_decoder:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1275\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_ids\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1270\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;124;03m    `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1275\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\n\u001b[0;32m   1276\u001b[0m     all_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(all_toks)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_ids\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1251\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;124;03m    `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m \n\u001b[0;32m   1249\u001b[0m \u001b[38;5;124;03m    Convert tokens of `tokenizers.AddedToken` type to string.\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens_extended\u001b[49m]\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_toks\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1266\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens_extended\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1264\u001b[0m set_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map_extended\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr_value \u001b[38;5;129;01min\u001b[39;00m set_attr\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m-> 1266\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m all_toks \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mlist\u001b[39m(attr_value) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr_value, (\u001b[38;5;28;43mlist\u001b[39;49m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m [attr_value])\n\u001b[0;32m   1267\u001b[0m all_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(OrderedDict\u001b[38;5;241m.\u001b[39mfromkeys(all_toks))\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_toks\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ab8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('t5_gec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704aecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r 't5_gec_model.zip' 't5_gec_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9f6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = '/Users/jakar/Documents/HDK_Tasks/Grammer_Error_Correction/t5_gec_model'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93afe2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He is moving here.', \"He's moving here.\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = 'He are moving here.'\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffbb0c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a student.']\n"
     ]
    }
   ],
   "source": [
    "text = 'I is a student'\n",
    "print(correct_grammar(text, num_return_sequences=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458066e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
