{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u7iMiMEdNez"
   },
   "source": [
    " **High level workflow**\n",
    " \n",
    "•\tTokenize the sentence using Spacy\n",
    "\n",
    "•\tCheck for spelling errors using Hunspell\n",
    "\n",
    "•\tFor all preposition, determiners & helper verbs, create a set of probable sentences\n",
    "\n",
    "•\tCreate a set of sentences with each word “masked”, deleted or an additional determiner, preposition or helper verb added\n",
    "\n",
    "•\tUsed BERT Masked Language Model to determine possible suggestions for masks\n",
    "\n",
    "•\tUse the GED model to select appropriate solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "d0be7sVJ8hFj",
    "outputId": "1444be73-79a0-4fba-b0c8-adde9af794a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.199)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.6.8)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.199 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.199)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch_pretrained_bert) (2.5.3)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch_pretrained_bert) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.199->boto3->pytorch_pretrained_bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# install pytorch_pretrained_bert the previous version of Pytorch-Transformers\n",
    "!pip install -U pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41MNRTbh7qBm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ne5HvQLD7rCU",
    "outputId": "0c788c2d-2e8b-467f-b250-1c136dc91dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to confirm that GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "af63QP2z7s7s"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-72xZeds8KNQ",
    "outputId": "bfa6ded0-ab3a-4966-fdf1-5bad70bd5122"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\jakar\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCaEFtjS7vSq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\jakar\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_GE\u001b[39m(sents):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (C:\\Users\\jakar\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def check_GE(sents):\n",
    "    \"\"\"Check of the input sentences have grammatical errors\n",
    "\n",
    "    :param list: list of sentences\n",
    "    :return: error, probabilities\n",
    "    :rtype: (boolean, (float, float))\n",
    "    \"\"\"\n",
    "    \n",
    "  # Create sentence) and label lists\n",
    "  # We need to add special tokens at the beginning and end of each sentence\n",
    "  # for BERT to work properly\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sents]\n",
    "    labels =[0]\n",
    "\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "  # Padding Sentences\n",
    "  # Set the maximum sequence length. The longest sequence in our training set\n",
    "  # is 47, but we'll leave room on the end anyway.\n",
    "  # In the original paper, the authors used a length of 512.\n",
    "    MAX_LEN = 128\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "  # Pad our input tokens\n",
    "    input_ids = pad_sequences(\n",
    "          [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"\n",
    "          )\n",
    "\n",
    "  # Index Numbers and Padding\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "      # pad sentences\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                                dtype =\"long\", truncating=\"post\",padding =\"post\")\n",
    "\n",
    "      # Attention masks\n",
    "      # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "      # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    prediction_inputs = torch.tensor(input_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    prediction_labels = torch.tensor(labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        logits = modelGED(prediction_inputs, token_type_ids=None, \n",
    "                          attention_mask=prediction_masks)\n",
    "\n",
    "      # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "      # label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "      # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "      # true_labels.append(label_ids)\n",
    "\n",
    "    #   print(predictions)\n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    #   print(flat_predictions)\n",
    "    prob_vals = flat_predictions\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "      # flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    #   print(flat_predictions)\n",
    "    return flat_predictions, prob_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZjdtFfmzC9i"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# CREDIT: https://stackoverflow.com/a/39225039\n",
    "#\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "  print(\"Trying to fetch {}\".format(destination))\n",
    "\n",
    "  def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "      if key.startswith('download_warning'):\n",
    "        return value\n",
    "\n",
    "    return None\n",
    "\n",
    "  def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "      for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n",
    "        if chunk: # filter out keep-alive new chunks\n",
    "          f.write(chunk)\n",
    "\n",
    "  URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "  session = requests.Session()\n",
    "\n",
    "  response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "  token = get_confirm_token(response)\n",
    "\n",
    "  if token:\n",
    "    params = { 'id' : id, 'confirm' : token }\n",
    "    response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "  save_response_content(response, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YglAn18CzxJW"
   },
   "outputs": [],
   "source": [
    "def progress_bar(some_iter):\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        return tqdm(some_iter)\n",
    "    except ModuleNotFoundError:\n",
    "        return some_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "N72u8k_FzGkB",
    "outputId": "4eeffeb8-7d5a-46f9-ae0a-9eb322c5a348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch ./bert-based-uncased-GED.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 502.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# load previously trained BERT Grammar Error Detection model\n",
    "\n",
    "# download from public google drive link\n",
    "download_file_from_google_drive(\"1al7v87aRxebSUCXrN2Sdd0jGUS0zZ3vn\", \"./bert-based-uncased-GED.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YdekFl7a7ftt",
    "outputId": "886c531f-fb89-4834-86a3-5814967d1de4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to C:\\Users\\jakar\\AppData\\Local\\Temp\\tmpypexuo8m\n",
      "100%|████████████████████████████████████████████████████████████████| 407873900/407873900 [04:47<00:00, 1419950.55B/s]\n",
      "INFO:pytorch_pretrained_bert.file_utils:copying C:\\Users\\jakar\\AppData\\Local\\Temp\\tmpypexuo8m to cache at C:\\Users\\jakar\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for C:\\Users\\jakar\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.file_utils:removing temp file C:\\Users\\jakar\\AppData\\Local\\Temp\\tmpypexuo8m\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\jakar\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\jakar\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\jakar\\AppData\\Local\\Temp\\tmpdjwet6b7\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '<'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m modelGED \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      6\u001b[0m                                                       num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# restore model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m modelGED\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-based-uncased-GED.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m modelGED\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m    712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:920\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 920\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "from pytorch_pretrained_bert import BertForSequenceClassification\n",
    "\n",
    "modelGED = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                      num_labels=2)\n",
    "\n",
    "# restore model\n",
    "modelGED.load_state_dict(torch.load('bert-based-uncased-GED.pth'))\n",
    "modelGED.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ECUddA_Y9KA3",
    "outputId": "e7955d41-fc21-43d5-e440-6d83e43d2128"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights) for Masked Language Model (MLM)\n",
    "model = BertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jrrIyrNv-cEF",
    "outputId": "470c5da5-6c96-4d16-939e-72d3e5b16bad"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizerLarge = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "p20iABcF9nYe",
    "outputId": "02e365e2-91b1-4273-a30e-c21883a25300"
   },
   "outputs": [],
   "source": [
    "# install the packages for Hunspell\n",
    "\n",
    "!sudo apt-get install libhunspell-1.6-0 libhunspell-dev\n",
    "!pip install cyhunspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "rG1ow8ew9n_Y",
    "outputId": "87d8ee05-ba36-4d0b-b91d-7e023530c339"
   },
   "outputs": [],
   "source": [
    "from hunspell import Hunspell\n",
    "import os\n",
    "\n",
    "# download the gn_GB dictionary for hunspell\n",
    "download_file_from_google_drive(\"1jC5BVF9iZ0gmRQNmDcZnhfFdEYv8RNok\", \"./en_GB-large.dic\")\n",
    "download_file_from_google_drive(\"1g8PO8kdw-YmyOY_HxjnJ5FfdJFX4bsPv\", \"./en_GB-large.aff\")\n",
    "\n",
    "gb = Hunspell(\"en_GB-large\", hunspell_data_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4WCEE35jJKQ"
   },
   "outputs": [],
   "source": [
    "# List of common determiners\n",
    "# det = [\"\", \"the\", \"a\", \"an\"]\n",
    "det = ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', \n",
    "       'her', 'its', 'our', 'their', 'all', 'both', 'half', 'either', 'neither', \n",
    "       'each', 'every', 'other', 'another', 'such', 'what', 'rather', 'quite']\n",
    "\n",
    "# List of common prepositions\n",
    "prep = [\"about\", \"at\", \"by\", \"for\", \"from\", \"in\", \"of\", \"on\", \"to\", \"with\", \n",
    "        \"into\", \"during\", \"including\", \"until\", \"against\", \"among\", \n",
    "        \"throughout\", \"despite\", \"towards\", \"upon\", \"concerning\"]\n",
    "\n",
    "# List of helping verbs\n",
    "helping_verbs = ['am', 'is', 'are', 'was', 'were', 'being', 'been', 'be', \n",
    "                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', \n",
    "                 'shall', 'should', 'may', 'might', 'must', 'can', 'could']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrGKWPZYNoOX"
   },
   "outputs": [],
   "source": [
    "# test sentences\n",
    "\n",
    "org_text = []\n",
    "org_text.append(\"They drank the pub .\")\n",
    "org_text.append(\"I am looking forway to see you soon .\")\n",
    "org_text.append(\"The cat sat at mat .\")\n",
    "org_text.append(\"Giant otters is an apex predator .\")\n",
    "org_text.append('There is no a doubt, tracking system has brought many benefits in this information age .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QojckfCt9OAd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def create_spelling_set(org_text):\n",
    "  \"\"\" Create a set of sentences which have possible corrected spellings\n",
    "  \"\"\"\n",
    "  \n",
    "  sent = org_text\n",
    "  sent = sent.lower()\n",
    "  sent = sent.strip().split()\n",
    "\n",
    "\n",
    "  nlp = spacy.load(\"en\")\n",
    "  proc_sent = nlp.tokenizer.tokens_from_list(sent)\n",
    "  nlp.tagger(proc_sent)\n",
    "\n",
    "  sentences = []\n",
    "\n",
    "  for tok in proc_sent:\n",
    "    # check for spelling for alphanumeric\n",
    "    if tok.text.isalpha() and not gb.spell(tok.text):\n",
    "      new_sent = sent[:]\n",
    "      # append new sentences with possible corrections\n",
    "      for sugg in gb.suggest(tok.text):\n",
    "        new_sent[tok.i] = sugg\n",
    "        sentences.append(\" \".join(new_sent))\n",
    "\n",
    "  spelling_sentences = sentences\n",
    "\n",
    "  # retain new sentences which have a \n",
    "  # minimum chance of correctness using BERT GED\n",
    "  new_sentences = []\n",
    "  \n",
    "  for sent in spelling_sentences:\n",
    "    no_error, prob_val = check_GE([sent])\n",
    "    exps = [np.exp(i) for i in prob_val[0]]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    if(softmax[1] > 0.6):\n",
    "      new_sentences.append(sent)\n",
    "  \n",
    "  \n",
    "  # if no corrections, append the original sentence\n",
    "  if len(spelling_sentences) == 0:\n",
    "    spelling_sentences.append(\" \".join(sent))\n",
    "\n",
    "  # eliminate dupllicates\n",
    "  [spelling_sentences.append(sent) for sent in new_sentences]\n",
    "  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
    "\n",
    "  return spelling_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7OdVrA9O7O0"
   },
   "outputs": [],
   "source": [
    "def create_grammar_set(spelling_sentences):\n",
    "  \"\"\" create a new set of sentences with deleted determiners, \n",
    "      prepositions & helping verbs\n",
    "      \n",
    "  \"\"\"\n",
    "  \n",
    "  new_sentences = []\n",
    "\n",
    "  for text in spelling_sentences:\n",
    "    sent = text.strip().split()\n",
    "    for i in range(len(sent)):\n",
    "      new_sent = sent[:]\n",
    "      \n",
    "      if new_sent[i] not in list(set(det + prep + helping_verbs)):\n",
    "        continue\n",
    "      \n",
    "      del new_sent[i]\n",
    "      text = \" \".join(new_sent)\n",
    "      \n",
    "      # retain new sentences which have a \n",
    "      # minimum chance of correctness using BERT GED\n",
    "      no_error, prob_val = check_GE([text])\n",
    "      exps = [np.exp(i) for i in prob_val[0]]\n",
    "      sum_of_exps = sum(exps)\n",
    "      softmax = [j/sum_of_exps for j in exps]\n",
    "      if(softmax[1] > 0.6):\n",
    "        new_sentences.append(text)\n",
    "  \n",
    "  # eliminate dupllicates\n",
    "  [spelling_sentences.append(sent) for sent in new_sentences]\n",
    "  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
    "  return spelling_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j34k7n2p9Y5q"
   },
   "outputs": [],
   "source": [
    "def create_mask_set(spelling_sentences):\n",
    "  \"\"\"For each input sentence create 2 sentences\n",
    "     (1) [MASK] each word\n",
    "     (2) [MASK] for each space between words\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "\n",
    "  for sent in spelling_sentences:\n",
    "    sent = sent.strip().split()\n",
    "    for i in range(len(sent)):\n",
    "      # (1) [MASK] each word\n",
    "      new_sent = sent[:]\n",
    "      new_sent[i] = '[MASK]'\n",
    "      text = \" \".join(new_sent)\n",
    "      new_sent = '[CLS] ' + text + ' [SEP]'\n",
    "      sentences.append(new_sent)\n",
    "\n",
    "      # (2) [MASK] for each space between words\n",
    "      new_sent = sent[:]\n",
    "      new_sent.insert(i, '[MASK]')\n",
    "      text = \" \".join(new_sent)\n",
    "      new_sent = '[CLS] ' + text + ' [SEP]'\n",
    "      sentences.append(new_sent)\n",
    "\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMC3vhj49ZjD"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def check_grammar(org_sent, sentences, spelling_sentences):\n",
    "  \"\"\" check grammar for the input sentences\n",
    "  \"\"\"\n",
    "  \n",
    "  n = len(sentences)\n",
    "  \n",
    "  # what is the tokenized value of [MASK]. Usually 103\n",
    "  text = '[MASK]'\n",
    "  tokenized_text = tokenizerLarge.tokenize(text)\n",
    "  mask_token = tokenizerLarge.convert_tokens_to_ids(tokenized_text)[0]\n",
    "\n",
    "  LM_sentences = []\n",
    "  new_sentences = []\n",
    "  i = 0 # current sentence number\n",
    "  l = len(org_sent.strip().split())*2 # l is no of sentencees\n",
    "  mask = False # flag indicating if we are processing space MASK\n",
    "\n",
    "  for sent in sentences:\n",
    "    i += 1\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    if i%50 == 0:\n",
    "      print(\"\")\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokenized_text = tokenizerLarge.tokenize(sent)\n",
    "    indexed_tokens = tokenizerLarge.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # index of the masked token\n",
    "    mask_index = (tokens_tensor == mask_token).nonzero()[0][1].item()\n",
    "    # predicted token\n",
    "    predicted_index = torch.argmax(predictions[0, mask_index]).item()\n",
    "    predicted_token = tokenizerLarge.convert_ids_to_tokens([predicted_index])[0]\n",
    "    \n",
    "    # second best prediction. Can you used to create more options\n",
    "#     second_index = torch.topk(predictions[0, mask_index], 2).indices[1].item()\n",
    "#     second_prediction = tokenizer.convert_ids_to_tokens([second_index])[0]\n",
    "\n",
    "    text = sent.strip().split()\n",
    "    mask_index = text.index('[MASK]')\n",
    "\n",
    "    if not mask:\n",
    "      # case of MASKed words\n",
    "      \n",
    "      mask = True\n",
    "      text[mask_index] = predicted_token\n",
    "      try:\n",
    "        # retrieve original word\n",
    "        org_word = spelling_sentences[i//l].strip().split()[mask_index-1]\n",
    "#         print(\">>> \" + org_word)\n",
    "      except:\n",
    "#         print(spelling_sentences[i%l - 1])\n",
    "#         print(tokenized_text)\n",
    "#         print(\"{0} {1} {2}\".format(i, l, mask_index))\n",
    "        print(\"!\", end=\"\")\n",
    "        continue\n",
    "  #     print(\"{0} - {1}\".format(org_word, predicted_token))\n",
    "      # check if the prediction is an inflection of the original word\n",
    "  #   if org_word.isalpha() and predicted_token not in gb_infl[org_word]:\n",
    "  #     continue\n",
    "      # use SequenceMatcher to see if predicted word is similar to original word\n",
    "      if SequenceMatcher(None, org_word, predicted_token).ratio() < 0.6:\n",
    "        if org_word not in list(set(det + prep + helping_verbs)) or predicted_token not in list(set(det + prep + helping_verbs)):\n",
    "          continue\n",
    "      if org_word == predicted_token:\n",
    "        continue\n",
    "    else:\n",
    "      # case for MASKed spaces\n",
    "      \n",
    "      mask = False\n",
    "  #     print(\"{0}\".format(predicted_token))\n",
    "      # only allow determiners / prepositions  / helping verbs in spaces\n",
    "      if predicted_token in list(set(det + prep + helping_verbs)) :\n",
    "        text[mask_index] = predicted_token\n",
    "      else:\n",
    "        continue\n",
    "\n",
    "  #   if org_word == \"in\":\n",
    "  #     print(\">>>>>> \" + predicted_token)\n",
    "  #   print(tokenized_text)\n",
    "  #   print(mask_index)\n",
    "  \n",
    "    text.remove('[SEP]')\n",
    "    text.remove('[CLS]')\n",
    "    new_sent = \" \".join(text)\n",
    "    \n",
    "  #   print(new_sent)\n",
    "    # retain new sentences which have a \n",
    "    # minimum chance of correctness using BERT GED\n",
    "    no_error, prob_val = check_GE([new_sent])\n",
    "    exps = [np.exp(i) for i in prob_val[0]]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    if no_error and softmax[1] > 0.996:\n",
    "  #     print(org_word)\n",
    "  #     print(predicted_token)\n",
    "  #     print(SequenceMatcher(None, org_word, predicted_token).ratio())\n",
    "  #     print(\"{0} - {1}, {2}\".format(prob_val[0][1], prob_val[0][0], prob_val[0][1] - prob_val[0][0]))\n",
    "\n",
    "  #     print(\"{0} - {1:.2f}\".format(new_sent, softmax[1]*100) )\n",
    "      print(\"*\", end=\"\")\n",
    "      new_sentences.append(new_sent)\n",
    "  #   print(\"{0}\\t{1}\".format(predicted_token, second_prediction))\n",
    "\n",
    "  print(\"\")\n",
    "  \n",
    "  # remove duplicate suggestions\n",
    "  spelling_sentences = []\n",
    "  [spelling_sentences.append(sent) for sent in new_sentences]\n",
    "  spelling_sentences = list(dict.fromkeys(spelling_sentences))\n",
    "  spelling_sentences\n",
    "  \n",
    "  return spelling_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "h5OizN4gFigm",
    "outputId": "9b696903-4c7d-4760-d71e-a395cfc19ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence >>> They drank the pub .\n",
      "processing 10 possibilities\n",
      "......*....\n",
      "Suggestions & Probabilities\n",
      "they drank at the pub . - 99.8071%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Input Sentence >>> I am looking forway to see you soon .\n",
      "processing 126 possibilities\n",
      ".......*..................*..................*.*......\n",
      "..............*.........!........*...................\n",
      "...*......!...*.*.*............\n",
      "Suggestions & Probabilities\n",
      "i am looking forward to see you soon . - 99.7493%\n",
      "i am looking to Norway to see you soon . - 99.7757%\n",
      "i am looking for a way to see you soon . - 99.7564%\n",
      "i am looking forward to seeing you soon . - 99.7722%\n",
      "am i looking forward to see you soon . - 99.6467%\n",
      "i look forward to see you soon . - 99.7621%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Input Sentence >>> The cat sat at mat .\n",
      "processing 12 possibilities\n",
      "..........*..\n",
      "Suggestions & Probabilities\n",
      "the cat sat at the mat . - 99.8284%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Input Sentence >>> Giant otters is an apex predator .\n",
      "processing 26 possibilities\n",
      ".....*...............*......\n",
      "Suggestions & Probabilities\n",
      "giant otters are an apex predator . - 99.8082%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Input Sentence >>> There is no a doubt, tracking system has brought many benefits in this information age .\n",
      "processing 152 possibilities\n",
      "..................................................\n",
      "......................*..*..........................\n",
      "..................................................\n",
      "..\n",
      "Suggestions & Probabilities\n",
      "there is no doubt, the tracking system has brought many benefits in this information age . - 99.6639%\n",
      "there is no doubt, tracking the system has brought many benefits in this information age . - 99.6644%\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# org_text = []\n",
    "# with open(\"./drive/My Drive/Colab Notebooks/S89A/CoNLL_2013_DS.txt\") as file:\n",
    "#   org_text = file.readlines()\n",
    "\n",
    "# predict for each of the test samples\n",
    "\n",
    "for sent in org_text:\n",
    "  \n",
    "  print(\"Input Sentence >>> \" + sent)\n",
    "  \n",
    "  sentences = create_spelling_set(sent)\n",
    "  spelling_sentences = create_grammar_set(sentences)\n",
    "  sentences = create_mask_set(spelling_sentences)\n",
    "  \n",
    "  print(\"processing {0} possibilities\".format(len(sentences)))\n",
    "  \n",
    "  sentences = check_grammar(sent, sentences, spelling_sentences)\n",
    "\n",
    "  print(\"Suggestions & Probabilities\")\n",
    "  \n",
    "  if len(sentences) == 0:\n",
    "    print(\"None\")\n",
    "    continue\n",
    "\n",
    "  no_error, prob_val =  check_GE(sentences)\n",
    "\n",
    "  for i in range(len(prob_val)):\n",
    "    exps = [np.exp(i) for i in prob_val[i]]\n",
    "    sum_of_exps = sum(exps)\n",
    "    softmax = [j/sum_of_exps for j in exps]\n",
    "    print(\"{0} - {1:0.4f}%\".format(sentences[i], softmax[1]*100))\n",
    "  \n",
    "  print(\"-\"*60)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_GEC_Implementation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
