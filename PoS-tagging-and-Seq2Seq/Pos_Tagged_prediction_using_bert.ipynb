{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmO6-9EbX_L3",
        "outputId": "06d715c1-5123-48bf-9d25-a977041197fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Iz2NxDG_jS2",
        "outputId": "6bab4937-830d-4701-dd68-81ae54a7b3b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-tensorflow==1.0.1\n",
            "  Downloading bert_tensorflow-1.0.1-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyconll\n",
        "! pip install pydot\n",
        "!pip install graphviz\n",
        "!pip install bert-tensorflow==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "drP5Ti36_mjS"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "import tensorflow as tf    \n",
        "tf.random.set_seed(1)\n",
        "from tensorflow import keras\n",
        "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile, collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from bert.tokenization import FullTokenizer\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from IPython.display import Image \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Up_a9bot_oUy"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2IC-z2af_4Xh"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(f1,\n",
        "                          cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True,\n",
        "                          i=1):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gX7Bcora_8fH"
      },
      "outputs": [],
      "source": [
        "def plot_acc():\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aRqd6IZ0AAbe"
      },
      "outputs": [],
      "source": [
        "def plot_loss():\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lUz32pKFABTO"
      },
      "outputs": [],
      "source": [
        "UD_ENGLISH_TRAIN = 'en_partut-ud-train.conllu'\n",
        "UD_ENGLISH_DEV = 'en_partut-ud-dev.conllu'\n",
        "UD_ENGLISH_TEST = 'en_partut-ud-test.conllu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h5N9DRPtAEBL"
      },
      "outputs": [],
      "source": [
        "from six.moves import urllib\n",
        "def download_files():\n",
        "    print('Downloading English treebank...')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-dev.conllu', 'en_partut-ud-dev.conllu')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-test.conllu', 'en_partut-ud-test.conllu')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-train.conllu', 'en_partut-ud-train.conllu')\n",
        "    print('Treebank downloaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px8VkhV0AGUy",
        "outputId": "d1e1ea72-5e8c-4346-facf-cdce06418697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading English treebank...\n",
            "Treebank downloaded.\n"
          ]
        }
      ],
      "source": [
        "download_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YmaTKZGDAIFv"
      },
      "outputs": [],
      "source": [
        "def read_conllu(path):\n",
        "    data = pyconll.load_from_file(path)\n",
        "    tagged_sentences=[]\n",
        "    t=0\n",
        "    for sentence in data:\n",
        "        tagged_sentence=[]\n",
        "        for token in sentence:\n",
        "            if token.upos and token.form:\n",
        "                t+=1\n",
        "                tagged_sentence.append((token.form.lower(), token.upos))\n",
        "        tagged_sentences.append(tagged_sentence)\n",
        "    return tagged_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lf9piCSaAYs-"
      },
      "outputs": [],
      "source": [
        "train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n",
        "val_sentences = read_conllu(UD_ENGLISH_DEV)\n",
        "test_sentences = read_conllu(UD_ENGLISH_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
        "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
        "print(40*'=')\n",
        "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
        "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
        "print(40*'=')\n",
        "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
        "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))\n",
        "print(40*'*')\n",
        "print(\"Total sentences in dataset:\", len(train_sentences)+len(val_sentences)+len(test_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th3wdtvdHesc",
        "outputId": "1d33eb3d-40d9-4575-fae2-383ddc54b244"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagged sentences in train set:  1781\n",
            "Tagged words in train set: 43518\n",
            "========================================\n",
            "Tagged sentences in dev set:  156\n",
            "Tagged words in dev set: 2722\n",
            "========================================\n",
            "Tagged sentences in test set:  153\n",
            "Tagged words in test set: 3408\n",
            "****************************************\n",
            "Total sentences in dataset: 2090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxPW5mfTHjIC",
        "outputId": "8c8353eb-d943-4cf5-abf5-8408dabd9e0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('distribution', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('license', 'NOUN'), ('does', 'AUX'), ('not', 'PART'), ('create', 'VERB'), ('an', 'DET'), ('attorney', 'NOUN'), ('-', 'PUNCT'), ('client', 'NOUN'), ('relationship', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_sequence(sentences):\n",
        "    return [[t for w, t in sentence] for sentence in sentences]\n",
        "\n",
        "def text_sequence(sentences):\n",
        "    return [[w for w, t in sentence] for sentence in sentences]"
      ],
      "metadata": {
        "id": "7NriXsSAHxIa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tags = set([item for sublist in train_sentences+test_sentences+val_sentences for _, item in sublist])\n",
        "print('TOTAL TAGS: ', len(tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYQTvJWHygK",
        "outputId": "836e16b1-024f-4403-a1b0-432ff505db59"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL TAGS:  17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag2int = {}\n",
        "int2tag = {}\n",
        "\n",
        "for i, tag in enumerate(sorted(tags)):\n",
        "    tag2int[tag] = i+1\n",
        "    int2tag[i+1] = tag"
      ],
      "metadata": {
        "id": "FQATReqyH2Ab"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2int['-PAD-'] = 0\n",
        "int2tag[0] = '-PAD-'"
      ],
      "metadata": {
        "id": "eT1n_QMyH5t7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_tags = len(tag2int)\n",
        "print('Total tags:', n_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsSRrwpZIAUT",
        "outputId": "01f56664-45ab-4c91-d4f3-6e02df502619"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tags: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(tag2int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJPSb8BdJC1q",
        "outputId": "ab5e06db-d0b3-4eb8-f385-6d9893d02323"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'AUX',\n",
              " 'CCONJ',\n",
              " 'DET',\n",
              " 'INTJ',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PART',\n",
              " 'PRON',\n",
              " 'PROPN',\n",
              " 'PUNCT',\n",
              " 'SCONJ',\n",
              " 'SYM',\n",
              " 'VERB',\n",
              " 'X',\n",
              " '-PAD-']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 70\n",
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "LoyQADrYJUbM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n",
        "val_sentences = read_conllu(UD_ENGLISH_DEV)\n",
        "test_sentences = read_conllu(UD_ENGLISH_TEST)"
      ],
      "metadata": {
        "id": "m6WreF5MJaUr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.hist([len(s) for s in train_sentences], bins=50)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Z30qDtyLJgrN",
        "outputId": "207a4de4-c7ef-44ba-e928-b9facd120c87"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Max sentence length:',len(max(train_sentences+val_sentences, key=len)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkQAE4DQJ8JF",
        "outputId": "e6199243-d320-4ce0-f36d-1577561c3086"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length: 209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def  split(sentences, max):\n",
        "    new=[]\n",
        "    for data in sentences:\n",
        "        new.append(([data[x:x+max] for x in range(0, len(data), max)]))\n",
        "    new = [val for sublist in new for val in sublist]\n",
        "    return new"
      ],
      "metadata": {
        "id": "duVFCbepK_OS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = split(train_sentences, MAX_SEQUENCE_LENGTH)\n",
        "val_sentences = split(val_sentences, MAX_SEQUENCE_LENGTH)\n",
        "test_sentences = split(test_sentences, MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "cG8rLFmuLDb6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(max(train_sentences+val_sentences, key=len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exmgeQDnLOur",
        "outputId": "70a8cd8a-bb52-42ba-9ea1-8b0b083c6617"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_sentences + val_sentences"
      ],
      "metadata": {
        "id": "Js6PVPHGLR0D"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sess = tf.compat.v1.Session()\n",
        "# Params for bert model and tokenization\n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/2\""
      ],
      "metadata": {
        "id": "rYvrxDkrLUtk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = text_sequence(train_sentences)\n",
        "test_text = text_sequence(test_sentences)\n",
        "#val_text = text_sequence(val_sentences)\n",
        "\n",
        "train_label = tag_sequence(train_sentences)\n",
        "test_label= tag_sequence(test_sentences)\n",
        "#val_label= tag_sequence(val_sentences)"
      ],
      "metadata": {
        "id": "R1R2maZbMtH8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(train_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWPtoT7lL6sa",
        "outputId": "0b1856d7-e33a-4301-823a-ed3443046dfb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1951"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text[0], train_label[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKTwqX9FMwaj",
        "outputId": "33c2beb0-74fa-468b-9ced-4c44d61dbcef"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['distribution',\n",
              "  'of',\n",
              "  'this',\n",
              "  'license',\n",
              "  'does',\n",
              "  'not',\n",
              "  'create',\n",
              "  'an',\n",
              "  'attorney',\n",
              "  '-',\n",
              "  'client',\n",
              "  'relationship',\n",
              "  '.'],\n",
              " ['NOUN',\n",
              "  'ADP',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'AUX',\n",
              "  'PART',\n",
              "  'VERB',\n",
              "  'DET',\n",
              "  'NOUN',\n",
              "  'PUNCT',\n",
              "  'NOUN',\n",
              "  'NOUN',\n",
              "  'PUNCT'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Params for bert model and tokenization\n",
        "# bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
        "    \n",
        "    vocab_file = bert_module.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_module.resolved_object.do_lower_case.numpy()\n",
        "    return FullTokenizer(vocab_file,  do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label_ids = [0] * max_seq_length\n",
        "        return input_ids, input_mask, segment_ids, label_ids\n",
        "    \n",
        "    tokens_a = example.text_a\n",
        "    if len(tokens_a) > max_seq_length-2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length-2)]\n",
        "\n",
        "# Token map will be an int -> int mapping between the `orig_tokens` index and\n",
        "# the `bert_tokens` index.\n",
        "\n",
        "# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n",
        "# orig_to_tok_map == [1, 2, 4, 6]   \n",
        "    orig_to_tok_map = []              \n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    \n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    orig_to_tok_map.append(len(tokens)-1)\n",
        "    #print(len(tokens_a))\n",
        "    for token in tokens_a:       \n",
        "        tokens.extend(tokenizer.tokenize(token))\n",
        "        orig_to_tok_map.append(len(tokens)-1)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "    orig_to_tok_map.append(len(tokens)-1)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\n",
        "    #print(len(orig_to_tok_map), len(tokens), len(input_ids), len(segment_ids)) #for debugging\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    \n",
        "    label_ids = []\n",
        "    labels = example.label\n",
        "    label_ids.append(0)\n",
        "    label_ids.extend([tag2int[label] for label in labels])\n",
        "    label_ids.append(0)\n",
        "    #print(len(label_ids)) #for debugging\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "        label_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "    assert len(label_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, label_ids\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples"
      ],
      "metadata": {
        "id": "kbuv3PUEMj27"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "LZkzR4JoM8Hj",
        "outputId": "086c3ad4-ca40-42bd-8d8f-06d7644d78fe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f3f249aee57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tokenizer_from_hub_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-655eaa9a1fa5>\u001b[0m in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_single_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# orig_tokens = [\"John\", \"Johanson\", \"'s\",  \"house\"]\n",
        "# labels      = [\"NNP\",  \"NNP\",      \"POS\", \"NN\"]\n",
        "# bert_tokens = ['[CLS]\", \"john\", \"johan\", \"##son\", \"'\",   \"s\",  \"house\", \"[SEP]\"]\n",
        "# bert_labels = ['[CLS]\", \"NNP\",  \"NNP\",   \"##\",    \"POS\", \"##\", \"NN\",    \"[SEP]\"] "
      ],
      "metadata": {
        "id": "LTCTRHGRNA2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_labels(labels):\n",
        "    train_label_bert = []\n",
        "    train_label_bert.append('-PAD-')\n",
        "    for i in labels:\n",
        "        train_label_bert.append(i)\n",
        "    train_label_bert.append('-PAD-')\n",
        "    print('BERT labels:', train_label_bert)  "
      ],
      "metadata": {
        "id": "rv9h9EOpRgLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens_a = train_text[2]"
      ],
      "metadata": {
        "id": "mKHhUXViRhzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_to_tok_map = []              \n",
        "tokens = []\n",
        "segment_ids = []\n",
        "tokens.append(\"[CLS]\")\n",
        "segment_ids.append(0)\n",
        "orig_to_tok_map.append(len(tokens)-1)\n",
        "for token in tokens_a:\n",
        "    #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\n",
        "    tokens.extend(tokenizer.tokenize(token))\n",
        "    orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\n",
        "    segment_ids.append(0)\n",
        "tokens.append(\"[SEP]\")\n",
        "segment_ids.append(0)\n",
        "orig_to_tok_map.append(len(tokens)-1)\n",
        "input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])"
      ],
      "metadata": {
        "id": "NsZrlm7_Rj9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ho2nlmQGm5sG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pos Tagged prediction using bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}