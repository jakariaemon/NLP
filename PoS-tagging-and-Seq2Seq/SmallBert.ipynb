{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmallBert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXTQZ3mToveA",
        "outputId": "34047917-4b10-43b6-97a2-0e99120a18bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.5 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboard==2.1.0\n",
            "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 11.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.46.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (3.3.7)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.1.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.1.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.1.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (3.2.0)\n",
            "Installing collected packages: tensorboard\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires tensorboard<2.9,>=2.8, but you have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install tensorboard==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "NbQ7Lq2WpCoy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "#For a kaggle username & key, just go to your kaggle account and generate key\n",
        "#The JSON file so downloaded contains both of them\n",
        "if(\"examine-the-examiner.zip\" not in os.listdir()):\n",
        "  print(\"Copy these two values from the JSON file so generated\")\n",
        "  os.environ['KAGGLE_USERNAME'] = getpass.getpass(prompt='Kaggle username: ') \n",
        "  os.environ['KAGGLE_KEY'] =  getpass.getpass(prompt='Kaggle key: ')\n",
        "  !kaggle datasets download -d therohk/examine-the-examiner\n",
        "  !unzip /content/examine-the-examiner.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyHT_HzSpEYo",
        "outputId": "c8e7ba6e-de62-4945-fbbd-55da2247a499"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy these two values from the JSON file so generated\n",
            "Kaggle username: ··········\n",
            "Kaggle key: ··········\n",
            "Downloading examine-the-examiner.zip to /content\n",
            " 75% 57.0M/75.7M [00:01<00:00, 38.5MB/s]\n",
            "100% 75.7M/75.7M [00:01<00:00, 42.5MB/s]\n",
            "Archive:  /content/examine-the-examiner.zip\n",
            "  inflating: examiner-date-text.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "def basicPreprocess(text):\n",
        "  try:\n",
        "    processed_text = text.lower()\n",
        "    processed_text = re.sub(r'\\W +', ' ', processed_text)\n",
        "  except Exception as e:\n",
        "    print(\"Exception:\",e,\",on text:\", text)\n",
        "    return None\n",
        "  return processed_text"
      ],
      "metadata": {
        "id": "7_6v5snKqu4E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "oraqRGNUqyjy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/examiner-date-text.csv\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YppFfNOq1mi",
        "outputId": "3a3df770-aa12-4940-9442-321e7b67d608"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         publish_date                                      headline_text\n",
            "0            20100101       100 Most Anticipated books releasing in 2010\n",
            "1            20100101       10 best films of 2009 - What's on your list?\n",
            "2            20100101  10 days of free admission at Lan Su Chinese Ga...\n",
            "3            20100101      10 PlayStation games to watch out for in 2010\n",
            "4            20100101  10 resolutions for a Happy New Year for you an...\n",
            "...               ...                                                ...\n",
            "3089776      20151231  Which is better investment, Lego bricks or gol...\n",
            "3089777      20151231  Wild score three unanswered goals to defeat th...\n",
            "3089778      20151231  With NASA and Russia on the sidelines, Europe ...\n",
            "3089779      20151231  Wolf Pack battling opponents, officials on the...\n",
            "3089780      20151231          Writespace hosts all genre open mic night\n",
            "\n",
            "[3089781 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1).sample(frac=1)\n",
        "data = data[:200000]"
      ],
      "metadata": {
        "id": "zN4hSM2rq4mq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbuTgew2q8jk",
        "outputId": "235e00fd-6319-40ca-b04d-8fc03ad20340"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         publish_date                                      headline_text\n",
            "1730786      20120315  Afghanistan: America shows the best and worst ...\n",
            "2760271      20140518  Savannah police and animal control guide air-d...\n",
            "2824465      20140815  Grief Awareness Day August 30, 2014; make it h...\n",
            "984583       20110118  Xbox 360 Failure: A Problem Not Properly Dealt...\n",
            "3028058      20150721  Can Sarah Palin make peace between Donald Trum...\n",
            "...               ...                                                ...\n",
            "2409529      20130605  Bynes job offer: Amanda Bynes turns down job a...\n",
            "1353948      20110728  Latest details on WWE's plans for Alberto Del ...\n",
            "1269253      20110608  Former St. Louis Ram Eric Crouch trying out fo...\n",
            "2918465      20141230  Columnist's informal survey finds some women h...\n",
            "410339       20100606  Free samples: Tea, One Touch diabetes monitor,...\n",
            "\n",
            "[200000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"headline_text\"] = data[\"headline_text\"].apply(basicPreprocess).dropna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P4Cu9xOq_NT",
        "outputId": "73872faf-d34e-4e0a-800f-8b2c02b4b71b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception: 'float' object has no attribute 'lower' ,on text: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSWHV0iMrCsz",
        "outputId": "26c410f6-0840-47c4-b02f-a1decb12d442"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         publish_date                                      headline_text\n",
            "1730786      20120315  afghanistan america shows the best and worst o...\n",
            "2760271      20140518  savannah police and animal control guide air-d...\n",
            "2824465      20140815  grief awareness day august 30 2014 make it hap...\n",
            "984583       20110118  xbox 360 failure a problem not properly dealt ...\n",
            "3028058      20150721  can sarah palin make peace between donald trum...\n",
            "...               ...                                                ...\n",
            "2409529      20130605  bynes job offer amanda bynes turns down job at...\n",
            "1353948      20110728  latest details on wwe's plans for alberto del ...\n",
            "1269253      20110608  former st louis ram eric crouch trying out for...\n",
            "2918465      20141230  columnist's informal survey finds some women h...\n",
            "410339       20100606  free samples tea one touch diabetes monitor mi...\n",
            "\n",
            "[200000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[\"headline_text\"]\n",
        "data = data.replace(\"\\n\",\" \")"
      ],
      "metadata": {
        "id": "qfRBzcvhrFv0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_files_dir = \"/tmp/text_split\"\n",
        "!mkdir {txt_files_dir}"
      ],
      "metadata": {
        "id": "dfZ4OFcOrIpH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "i=0\n",
        "for row in tqdm(data.to_list()):\n",
        "  file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
        "  try:\n",
        "    f = open(file_name, 'w')\n",
        "    f.write(row)\n",
        "    f.close()\n",
        "  except Exception as e:  #catch exceptions(for eg. empty rows)\n",
        "    print(row, e) \n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keC0fGh-rM4V",
        "outputId": "b0dfeda6-f90f-4eef-a8e2-f82a31680f92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 37769/200000 [00:03<00:17, 9167.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan write() argument must be str, not float\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200000/200000 [00:17<00:00, 11411.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "paths = [str(x) for x in Path(txt_files_dir).glob(\"**/*.txt\")]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "vocab_size=5000\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=vocab_size, min_frequency=5, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "metadata": {
        "id": "Gs7Jlp5urQg9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_data_dir = \"/tmp/lm_data\"\n",
        "!mkdir {lm_data_dir}"
      ],
      "metadata": {
        "id": "ZvpRfLANrWCM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = 0.9\n",
        "train_data_size = int(len(data)*train_split)\n",
        "\n",
        "with open(os.path.join(lm_data_dir,'train.txt') , 'w') as f:\n",
        "    for item in data[:train_data_size].tolist():\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open(os.path.join(lm_data_dir,'eval.txt') , 'w') as f:\n",
        "    for item in data[train_data_size:].tolist():\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "ZX8m37qbrf6q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/models\n",
        "!mkdir /content/models/smallBERTa"
      ],
      "metadata": {
        "id": "CVxfgOmCrgtK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model(\"/content/models/smallBERTa\", \"smallBERTa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PZcoh8hrmSa",
        "outputId": "c071fac0-1074-47a8-fc3b-9c4c295c31a3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/models/smallBERTa/smallBERTa-vocab.json',\n",
              " '/content/models/smallBERTa/smallBERTa-merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/models/smallBERTa/smallBERTa-vocab.json /content/models/smallBERTa/vocab.json\n",
        "!mv /content/models/smallBERTa/smallBERTa-merges.txt /content/models/smallBERTa/merges.txt"
      ],
      "metadata": {
        "id": "SxiBYBdQrpO7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_path = os.path.join(lm_data_dir,\"train.txt\")\n",
        "eval_path = os.path.join(lm_data_dir,\"eval.txt\")"
      ],
      "metadata": {
        "id": "VIAWuvePvqP7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.3,\n",
        "  \"hidden_size\": 128,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"num_attention_heads\": 1,\n",
        "  \"num_hidden_layers\": 1,\n",
        "  \"vocab_size\": vocab_size,\n",
        "  \"intermediate_size\": 256,\n",
        "  \"max_position_embeddings\": 256,\n",
        "  \"model_type\": \"roberta\"\n",
        "  }\n",
        "with open(\"/content/models/smallBERTa/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)"
      ],
      "metadata": {
        "id": "_Ja27sWjvtZC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers/tree/v2.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxY4OZMavymb",
        "outputId": "6e89ac2e-0928-4c55-ebb6-c7a05f8b7839"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'v2.5.0'...\n",
            "fatal: repository 'https://github.com/huggingface/transformers/tree/v2.5.0/' not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQkGc2r4v4Qc",
        "outputId": "11ef1a95-adc9-4a4a-afea-18024bbb61cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  6 05:51:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"train_path\"] = train_path\n",
        "os.environ[\"eval_path\"] = eval_path\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'  #Makes for easier debugging (just in case)\n",
        "weights_dir = \"/content/models/smallBERTa/weights\"\n",
        "!mkdir {weights_dir}"
      ],
      "metadata": {
        "id": "6s27tV81v_Ob"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = '''python /content/transformers/examples/legacy/run_language_modeling.py --output_dir {0}  \\\n",
        "    --model_type roberta \\\n",
        "    --mlm \\\n",
        "    --train_data_file {1} \\\n",
        "    --eval_data_file {2} \\\n",
        "    --config_name /content/models/smallBERTa \\\n",
        "    --tokenizer_name /content/models/smallBERTa \\\n",
        "    --do_train \\\n",
        "    --line_by_line \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_eval \\\n",
        "    --block_size 256 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --save_total_limit 2 \\\n",
        "    --save_steps 2000 \\\n",
        "    --logging_steps 500 \\\n",
        "    --per_gpu_eval_batch_size 32 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    '''.format(weights_dir, train_path, eval_path)\n"
      ],
      "metadata": {
        "id": "Ai5X55GiwoIy"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!{cmd}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh7VZY9Uwuot",
        "outputId": "3341b212-cb52-4dad-831a-1c1b2a3c7d25"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/06/2022 06:28:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/06/2022 06:28:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0001,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/models/smallBERTa/weights/runs/Jul06_06-28-27_0a85a9e44a18,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/models/smallBERTa/weights,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/models/smallBERTa/weights,\n",
            "save_on_each_node=False,\n",
            "save_steps=2000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:657] 2022-07-06 06:28:27,097 >> loading configuration file /content/models/smallBERTa/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-07-06 06:28:27,098 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/smallBERTa\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 5000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:392] 2022-07-06 06:28:27,098 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:657] 2022-07-06 06:28:27,098 >> loading configuration file /content/models/smallBERTa/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-07-06 06:28:27,099 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/smallBERTa\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 5000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1701] 2022-07-06 06:28:27,107 >> Didn't find file /content/models/smallBERTa/tokenizer.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1701] 2022-07-06 06:28:27,107 >> Didn't find file /content/models/smallBERTa/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1701] 2022-07-06 06:28:27,108 >> Didn't find file /content/models/smallBERTa/special_tokens_map.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1701] 2022-07-06 06:28:27,108 >> Didn't find file /content/models/smallBERTa/tokenizer_config.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file /content/models/smallBERTa/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file /content/models/smallBERTa/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1779] 2022-07-06 06:28:27,108 >> loading file None\n",
            "[INFO|configuration_utils.py:657] 2022-07-06 06:28:27,108 >> loading configuration file /content/models/smallBERTa/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-07-06 06:28:27,109 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/smallBERTa\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 5000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:657] 2022-07-06 06:28:27,118 >> loading configuration file /content/models/smallBERTa/config.json\n",
            "[INFO|configuration_utils.py:708] 2022-07-06 06:28:27,118 >> Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/models/smallBERTa\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.3,\n",
            "  \"hidden_size\": 128,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 256,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 1,\n",
            "  \"num_hidden_layers\": 1,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 5000\n",
            "}\n",
            "\n",
            "07/06/2022 06:28:27 - INFO - __main__ - Training new model from scratch\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:963: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "[INFO|language_modeling.py:132] 2022-07-06 06:28:27,165 >> Creating features from dataset file at /tmp/lm_data/train.txt\n",
            "[INFO|language_modeling.py:132] 2022-07-06 06:28:36,038 >> Creating features from dataset file at /tmp/lm_data/eval.txt\n",
            "[WARNING|training_args.py:1230] 2022-07-06 06:28:51,005 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1374: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[WARNING|training_args.py:1230] 2022-07-06 06:28:51,007 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1516] 2022-07-06 06:28:51,009 >> ***** Running training *****\n",
            "[INFO|trainer.py:1517] 2022-07-06 06:28:51,009 >>   Num examples = 180000\n",
            "[INFO|trainer.py:1518] 2022-07-06 06:28:51,009 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1519] 2022-07-06 06:28:51,009 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1520] 2022-07-06 06:28:51,009 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1521] 2022-07-06 06:28:51,009 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1522] 2022-07-06 06:28:51,009 >>   Total optimization steps = 28125\n",
            "[WARNING|training_args.py:1230] 2022-07-06 06:28:51,017 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1244] 2022-07-06 06:28:51,017 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 7.8072, 'learning_rate': 9.822222222222223e-05, 'epoch': 0.09}\n",
            "{'loss': 7.4866, 'learning_rate': 9.644444444444445e-05, 'epoch': 0.18}\n",
            "{'loss': 7.4262, 'learning_rate': 9.466666666666667e-05, 'epoch': 0.27}\n",
            "{'loss': 7.3888, 'learning_rate': 9.28888888888889e-05, 'epoch': 0.36}\n",
            "  7% 2000/28125 [00:38<14:17, 30.47it/s][INFO|trainer.py:2503] 2022-07-06 06:29:29,610 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-2000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:29:29,611 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:29:29,637 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 7.3266, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}\n",
            "{'loss': 7.2923, 'learning_rate': 8.933333333333334e-05, 'epoch': 0.53}\n",
            "{'loss': 7.2652, 'learning_rate': 8.755555555555556e-05, 'epoch': 0.62}\n",
            "{'loss': 7.2344, 'learning_rate': 8.577777777777777e-05, 'epoch': 0.71}\n",
            " 14% 4000/28125 [01:20<08:22, 47.97it/s][INFO|trainer.py:2503] 2022-07-06 06:30:11,407 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-4000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:30:11,408 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:30:11,416 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 7.2132, 'learning_rate': 8.4e-05, 'epoch': 0.8}\n",
            "{'loss': 7.162, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}\n",
            "{'loss': 7.1647, 'learning_rate': 8.044444444444444e-05, 'epoch': 0.98}\n",
            "{'loss': 7.1221, 'learning_rate': 7.866666666666666e-05, 'epoch': 1.07}\n",
            " 21% 6000/28125 [02:01<07:40, 48.05it/s][INFO|trainer.py:2503] 2022-07-06 06:30:52,697 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-6000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:30:52,697 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:30:52,707 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:30:52,724 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-2000] due to args.save_total_limit\n",
            "{'loss': 7.1071, 'learning_rate': 7.688888888888889e-05, 'epoch': 1.16}\n",
            "{'loss': 7.0837, 'learning_rate': 7.511111111111111e-05, 'epoch': 1.24}\n",
            "{'loss': 7.0492, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}\n",
            "{'loss': 7.0405, 'learning_rate': 7.155555555555555e-05, 'epoch': 1.42}\n",
            " 28% 8000/28125 [02:44<07:08, 46.98it/s][INFO|trainer.py:2503] 2022-07-06 06:31:35,426 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-8000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:31:35,427 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:31:35,436 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:31:35,453 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-4000] due to args.save_total_limit\n",
            "{'loss': 7.0242, 'learning_rate': 6.977777777777779e-05, 'epoch': 1.51}\n",
            "{'loss': 7.005, 'learning_rate': 6.800000000000001e-05, 'epoch': 1.6}\n",
            "{'loss': 6.9859, 'learning_rate': 6.622222222222224e-05, 'epoch': 1.69}\n",
            "{'loss': 6.9845, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}\n",
            " 36% 10000/28125 [03:28<06:25, 47.07it/s][INFO|trainer.py:2503] 2022-07-06 06:32:19,028 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-10000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:32:19,029 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:32:19,038 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:32:19,055 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 6.9621, 'learning_rate': 6.266666666666667e-05, 'epoch': 1.87}\n",
            "{'loss': 6.9499, 'learning_rate': 6.08888888888889e-05, 'epoch': 1.96}\n",
            "{'loss': 6.9428, 'learning_rate': 5.911111111111112e-05, 'epoch': 2.04}\n",
            "{'loss': 6.9256, 'learning_rate': 5.7333333333333336e-05, 'epoch': 2.13}\n",
            " 43% 12000/28125 [04:10<05:36, 47.92it/s][INFO|trainer.py:2503] 2022-07-06 06:33:01,444 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-12000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:33:01,445 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:33:01,453 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:33:01,471 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-8000] due to args.save_total_limit\n",
            "{'loss': 6.9012, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}\n",
            "{'loss': 6.8902, 'learning_rate': 5.377777777777778e-05, 'epoch': 2.31}\n",
            "{'loss': 6.886, 'learning_rate': 5.2000000000000004e-05, 'epoch': 2.4}\n",
            "{'loss': 6.87, 'learning_rate': 5.0222222222222226e-05, 'epoch': 2.49}\n",
            " 50% 14000/28125 [04:52<04:59, 47.09it/s][INFO|trainer.py:2503] 2022-07-06 06:33:44,001 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-14000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:33:44,002 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:33:44,010 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:33:44,028 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-10000] due to args.save_total_limit\n",
            "{'loss': 6.8938, 'learning_rate': 4.844444444444445e-05, 'epoch': 2.58}\n",
            "{'loss': 6.8449, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}\n",
            "{'loss': 6.8488, 'learning_rate': 4.4888888888888894e-05, 'epoch': 2.76}\n",
            "{'loss': 6.8297, 'learning_rate': 4.311111111111111e-05, 'epoch': 2.84}\n",
            " 57% 16000/28125 [05:36<04:25, 45.65it/s][INFO|trainer.py:2503] 2022-07-06 06:34:27,716 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-16000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:34:27,717 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:34:27,725 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:34:27,745 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-12000] due to args.save_total_limit\n",
            "{'loss': 6.8182, 'learning_rate': 4.133333333333333e-05, 'epoch': 2.93}\n",
            "{'loss': 6.8293, 'learning_rate': 3.9555555555555556e-05, 'epoch': 3.02}\n",
            "{'loss': 6.8089, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}\n",
            "{'loss': 6.805, 'learning_rate': 3.6e-05, 'epoch': 3.2}\n",
            " 64% 18000/28125 [06:19<03:38, 46.34it/s][INFO|trainer.py:2503] 2022-07-06 06:35:10,511 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-18000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:35:10,512 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:35:10,521 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:35:10,540 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-14000] due to args.save_total_limit\n",
            "{'loss': 6.7898, 'learning_rate': 3.4222222222222224e-05, 'epoch': 3.29}\n",
            "{'loss': 6.8078, 'learning_rate': 3.2444444444444446e-05, 'epoch': 3.38}\n",
            "{'loss': 6.7929, 'learning_rate': 3.066666666666667e-05, 'epoch': 3.47}\n",
            "{'loss': 6.7743, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}\n",
            " 71% 20000/28125 [07:02<02:56, 45.92it/s][INFO|trainer.py:2503] 2022-07-06 06:35:53,339 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-20000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:35:53,340 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:35:53,349 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:35:53,370 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-16000] due to args.save_total_limit\n",
            "{'loss': 6.764, 'learning_rate': 2.7111111111111114e-05, 'epoch': 3.64}\n",
            "{'loss': 6.7532, 'learning_rate': 2.5333333333333337e-05, 'epoch': 3.73}\n",
            "{'loss': 6.7686, 'learning_rate': 2.3555555555555556e-05, 'epoch': 3.82}\n",
            "{'loss': 6.7792, 'learning_rate': 2.177777777777778e-05, 'epoch': 3.91}\n",
            " 78% 22000/28125 [07:45<02:12, 46.39it/s][INFO|trainer.py:2503] 2022-07-06 06:36:36,212 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-22000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:36:36,213 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:36:36,221 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:36:36,239 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-18000] due to args.save_total_limit\n",
            "{'loss': 6.7666, 'learning_rate': 2e-05, 'epoch': 4.0}\n",
            "{'loss': 6.7403, 'learning_rate': 1.8222222222222224e-05, 'epoch': 4.09}\n",
            "{'loss': 6.7568, 'learning_rate': 1.6444444444444447e-05, 'epoch': 4.18}\n",
            "{'loss': 6.7358, 'learning_rate': 1.4666666666666668e-05, 'epoch': 4.27}\n",
            " 85% 24000/28125 [08:28<01:31, 45.02it/s][INFO|trainer.py:2503] 2022-07-06 06:37:19,485 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-24000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:37:19,485 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:37:19,494 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:37:19,511 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-20000] due to args.save_total_limit\n",
            "{'loss': 6.7259, 'learning_rate': 1.2888888888888889e-05, 'epoch': 4.36}\n",
            "{'loss': 6.7202, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}\n",
            "{'loss': 6.7448, 'learning_rate': 9.333333333333334e-06, 'epoch': 4.53}\n",
            "{'loss': 6.7254, 'learning_rate': 7.555555555555556e-06, 'epoch': 4.62}\n",
            " 92% 26000/28125 [09:11<00:45, 47.07it/s][INFO|trainer.py:2503] 2022-07-06 06:38:02,809 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-26000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:38:02,810 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:38:02,819 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:38:02,836 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-22000] due to args.save_total_limit\n",
            "{'loss': 6.718, 'learning_rate': 5.777777777777778e-06, 'epoch': 4.71}\n",
            "{'loss': 6.7457, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.8}\n",
            "{'loss': 6.7244, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}\n",
            "{'loss': 6.7227, 'learning_rate': 4.444444444444445e-07, 'epoch': 4.98}\n",
            "100% 28000/28125 [09:55<00:02, 46.38it/s][INFO|trainer.py:2503] 2022-07-06 06:38:46,063 >> Saving model checkpoint to /content/models/smallBERTa/weights/checkpoint-28000\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:38:46,064 >> Configuration saved in /content/models/smallBERTa/weights/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:38:46,075 >> Model weights saved in /content/models/smallBERTa/weights/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|trainer.py:2581] 2022-07-06 06:38:46,092 >> Deleting older checkpoint [/content/models/smallBERTa/weights/checkpoint-24000] due to args.save_total_limit\n",
            "100% 28125/28125 [09:57<00:00, 45.86it/s][INFO|trainer.py:1761] 2022-07-06 06:38:48,787 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 597.7777, 'train_samples_per_second': 1505.576, 'train_steps_per_second': 47.049, 'train_loss': 6.949547936197916, 'epoch': 5.0}\n",
            "100% 28125/28125 [09:57<00:00, 47.05it/s]\n",
            "[INFO|trainer.py:2503] 2022-07-06 06:38:48,788 >> Saving model checkpoint to /content/models/smallBERTa/weights\n",
            "[INFO|configuration_utils.py:446] 2022-07-06 06:38:48,789 >> Configuration saved in /content/models/smallBERTa/weights/config.json\n",
            "[INFO|modeling_utils.py:1660] 2022-07-06 06:38:48,797 >> Model weights saved in /content/models/smallBERTa/weights/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2123] 2022-07-06 06:38:48,799 >> tokenizer config file saved in /content/models/smallBERTa/weights/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2130] 2022-07-06 06:38:48,799 >> Special tokens file saved in /content/models/smallBERTa/weights/special_tokens_map.json\n",
            "07/06/2022 06:38:48 - INFO - __main__ - *** Evaluate ***\n",
            "[WARNING|training_args.py:1244] 2022-07-06 06:38:48,807 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[WARNING|training_args.py:1244] 2022-07-06 06:38:48,807 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:2753] 2022-07-06 06:38:48,807 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2755] 2022-07-06 06:38:48,807 >>   Num examples = 20000\n",
            "[INFO|trainer.py:2758] 2022-07-06 06:38:48,807 >>   Batch size = 32\n",
            " 99% 618/625 [00:05<00:00, 105.89it/s][WARNING|training_args.py:1244] 2022-07-06 06:38:54,744 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "100% 625/625 [00:05<00:00, 105.41it/s]\n",
            "07/06/2022 06:38:54 - INFO - __main__ - ***** Eval results *****\n",
            "07/06/2022 06:38:54 - INFO - __main__ -   perplexity = 751.2023186588478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DpLDUv2CI8kj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}